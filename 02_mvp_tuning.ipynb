{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on Pytorch Geometric official example: https://github.com/pyg-team/pytorch_geometric/blob/master/examples/lightgcn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_245289/4266816003.py:12: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import datetime as dt\n",
    "import itertools as it\n",
    "import functools as ft\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "from tqdm.notebook import tqdm # Progress bars\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "\n",
    "# https://import-as.github.io\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric as PyG\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from ray import tune\n",
    "from ray.air import Checkpoint, session\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import src\n",
    "from src.data import get_df, filter_df\n",
    "\n",
    "RANDOM_SEED = 1701"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters table in [Google Drive](https://docs.google.com/spreadsheets/d/1riafpWt1563w9pbqdt1g2QZVkc7TfRWGzFaCG5rudDI/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Remove users with less than 6 votes from the dataset before splitting\n",
    "DatasetConfig = namedtuple('DatasetConfig', ('min_votes_per_user', 'allowed_dao_names', 'num_folds'))\n",
    "datasetConfig = DatasetConfig(\n",
    "    min_votes_per_user=6,\n",
    "    allowed_dao_names={'dxDAO', 'xDXdao'},\n",
    "    num_folds=5,\n",
    ")\n",
    "\n",
    "ModelConfig = namedtuple('ModelConfig', 'max_epochs batch_size learning_rate embedding_dim conv_layers l2')\n",
    "modelConfig = ModelConfig(\n",
    "    max_epochs=50,\n",
    "    batch_size=16,\n",
    "    learning_rate=0.0001,\n",
    "    embedding_dim=32,\n",
    "    conv_layers=3,\n",
    "    l2=1e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges:          16606\n",
      "Density:       0.3087%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  voter={ num_nodes=104 },\n",
       "  proposal={ num_nodes=2216 },\n",
       "  (voter, votes, proposal)={ edge_index=[2, 8303] },\n",
       "  (proposal, voted, voter)={ edge_index=[2, 8303] }\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, HeteroData, Data\n",
    "from src.datasets import Daostack\n",
    "\n",
    "def print_graph_stats(g: HeteroData):\n",
    "    density = (g.num_edges) / (g.num_nodes*(g.num_nodes-1))\n",
    "    print(f'Edges:   {g.num_edges:12}')\n",
    "    print(f'Density: {density*100:12.4f}%')\n",
    "\n",
    "data = Daostack(\"./data/dao-analyzer/\", min_vpu=datasetConfig.min_votes_per_user, allowed_daos=datasetConfig.allowed_dao_names)[0]\n",
    "print_graph_stats(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, I thought the RandomLinkSplit function was not working properly, but it turns out that I wasn't understanding it very well. The tutorial I used for [01_mvp](./01_mvp.ipynb) is not very good either, it was written by students, and implemented before PyTorch Geometric bundled the LightGCN model with it.\n",
    "\n",
    "> I think this is totally correct. It seems like you are looking at the shapes of edge_index, while you may want to look at the shapes of edge_label and edge_label_index (which correctly model a 80/10/10 split ratio). Here, edge_index is solely used for message passing, i.e.,\n",
    "> \n",
    "> * for training, we exchange messages on all training edges\n",
    "> * for validation, we exchange messages on all training edges\n",
    "> * for testing, we exchange messages on all training and validation edges\n",
    "> Let me know if this resolves your concerns :)\n",
    ">\n",
    "> -- [Split Error in RandomLinkSplit · Issue #3668 · pyg-team/pytorch_geometric · GitHub](https://github.com/pyg-team/pytorch_geometric/issues/3668)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(HeteroData(\n",
       "   voter={ num_nodes=104 },\n",
       "   proposal={ num_nodes=2216 },\n",
       "   (voter, votes, proposal)={\n",
       "     edge_index=[2, 7266],\n",
       "     edge_label=[14532],\n",
       "     edge_label_index=[2, 14532],\n",
       "   },\n",
       "   (proposal, voted, voter)={ edge_index=[2, 7266] }\n",
       " ),\n",
       " HeteroData(\n",
       "   voter={ num_nodes=104 },\n",
       "   proposal={ num_nodes=2216 },\n",
       "   (voter, votes, proposal)={\n",
       "     edge_index=[2, 7266],\n",
       "     edge_label=[2074],\n",
       "     edge_label_index=[2, 2074],\n",
       "   },\n",
       "   (proposal, voted, voter)={ edge_index=[2, 7266] }\n",
       " ),\n",
       " HeteroData(\n",
       "   voter={ num_nodes=104 },\n",
       "   proposal={ num_nodes=2216 },\n",
       "   (voter, votes, proposal)={\n",
       "     edge_index=[2, 8303],\n",
       "     edge_label=[0],\n",
       "     edge_label_index=[2, 0],\n",
       "   },\n",
       "   (proposal, voted, voter)={ edge_index=[2, 8303] }\n",
       " ))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_train_val_test(g: Data | HeteroData, train_ratio=0.75):\n",
    "    t = ft.partial(PyG.transforms.RandomLinkSplit, \n",
    "        is_undirected=True,\n",
    "        num_val=1-train_ratio,\n",
    "        # split_labels=True,\n",
    "        add_negative_train_samples=True,\n",
    "        num_test=0,\n",
    "    )\n",
    "    \n",
    "    if isinstance(g, HeteroData):\n",
    "        t = t(\n",
    "            edge_types=[g.edge_types[0]],\n",
    "            rev_edge_types=[g.edge_types[1]] if len(g.edge_types) > 1 else None,\n",
    "        )\n",
    "    elif isinstance(g, Data):\n",
    "        t = t()\n",
    "            \n",
    "    return t(g)\n",
    "\n",
    "tr, val, ts = get_train_val_test(data, train_ratio=7/8)\n",
    "tr, val, ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(HeteroData(\n",
       "    voter={ num_nodes=104 },\n",
       "    proposal={ num_nodes=2216 },\n",
       "    (voter, votes, proposal)={\n",
       "      edge_index=[2, 6642],\n",
       "      negative_samples=[2, 6642],\n",
       "    },\n",
       "    (proposal, voted, voter)={ edge_index=[2, 6642] }\n",
       "  ),\n",
       "  HeteroData(\n",
       "    voter={ num_nodes=104 },\n",
       "    proposal={ num_nodes=2216 },\n",
       "    (voter, votes, proposal)={ edge_index=[2, 1661] },\n",
       "    (proposal, voted, voter)={ edge_index=[2, 1661] }\n",
       "  )),\n",
       " (HeteroData(\n",
       "    voter={ num_nodes=104 },\n",
       "    proposal={ num_nodes=2216 },\n",
       "    (voter, votes, proposal)={\n",
       "      edge_index=[2, 6642],\n",
       "      negative_samples=[2, 6642],\n",
       "    },\n",
       "    (proposal, voted, voter)={ edge_index=[2, 6642] }\n",
       "  ),\n",
       "  HeteroData(\n",
       "    voter={ num_nodes=104 },\n",
       "    proposal={ num_nodes=2216 },\n",
       "    (voter, votes, proposal)={ edge_index=[2, 1661] },\n",
       "    (proposal, voted, voter)={ edge_index=[2, 1661] }\n",
       "  )),\n",
       " (HeteroData(\n",
       "    voter={ num_nodes=104 },\n",
       "    proposal={ num_nodes=2216 },\n",
       "    (voter, votes, proposal)={\n",
       "      edge_index=[2, 6642],\n",
       "      negative_samples=[2, 6642],\n",
       "    },\n",
       "    (proposal, voted, voter)={ edge_index=[2, 6642] }\n",
       "  ),\n",
       "  HeteroData(\n",
       "    voter={ num_nodes=104 },\n",
       "    proposal={ num_nodes=2216 },\n",
       "    (voter, votes, proposal)={ edge_index=[2, 1661] },\n",
       "    (proposal, voted, voter)={ edge_index=[2, 1661] }\n",
       "  )),\n",
       " (HeteroData(\n",
       "    voter={ num_nodes=104 },\n",
       "    proposal={ num_nodes=2216 },\n",
       "    (voter, votes, proposal)={\n",
       "      edge_index=[2, 6643],\n",
       "      negative_samples=[2, 6643],\n",
       "    },\n",
       "    (proposal, voted, voter)={ edge_index=[2, 6643] }\n",
       "  ),\n",
       "  HeteroData(\n",
       "    voter={ num_nodes=104 },\n",
       "    proposal={ num_nodes=2216 },\n",
       "    (voter, votes, proposal)={ edge_index=[2, 1660] },\n",
       "    (proposal, voted, voter)={ edge_index=[2, 1660] }\n",
       "  )),\n",
       " (HeteroData(\n",
       "    voter={ num_nodes=104 },\n",
       "    proposal={ num_nodes=2216 },\n",
       "    (voter, votes, proposal)={\n",
       "      edge_index=[2, 6643],\n",
       "      negative_samples=[2, 6643],\n",
       "    },\n",
       "    (proposal, voted, voter)={ edge_index=[2, 6643] }\n",
       "  ),\n",
       "  HeteroData(\n",
       "    voter={ num_nodes=104 },\n",
       "    proposal={ num_nodes=2216 },\n",
       "    (voter, votes, proposal)={ edge_index=[2, 1660] },\n",
       "    (proposal, voted, voter)={ edge_index=[2, 1660] }\n",
       "  ))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def graph_k_fold(g: Data | HeteroData, folds, edge_type=None):\n",
    "    skf = StratifiedKFold(folds, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "    folds = []\n",
    "\n",
    "    # Stratify by voter\n",
    "    if edge_type is None:\n",
    "        edge_type = g.edge_types[0]\n",
    "        rev_edge_type = g.edge_types[1]\n",
    "        \n",
    "    edge_index = g[edge_type].edge_index\n",
    "    for train_idx, val_idx in skf.split(torch.zeros(edge_index.size(1)), edge_index[0]):\n",
    "        gtrain = g.edge_subgraph({\n",
    "            edge_type:torch.tensor(train_idx),\n",
    "            rev_edge_type:torch.tensor(train_idx),\n",
    "        })\n",
    "        assert gtrain.is_undirected()\n",
    "        assert len(gtrain[edge_type].edge_index[0].unique()) == len(g[edge_type].edge_index[0].unique())\n",
    "        gtrain[edge_type].negative_samples = PyG.utils.negative_sampling(gtrain[edge_type].edge_index, num_nodes=gtrain.num_nodes)\n",
    "        gval = g.edge_subgraph({\n",
    "            edge_type:torch.tensor(val_idx),\n",
    "            rev_edge_type:torch.tensor(val_idx),\n",
    "        })\n",
    "        assert gval.is_undirected()\n",
    "        assert len(gval[edge_type].edge_index[0].unique()) == len(g[edge_type].edge_index[0].unique())\n",
    "        assert (gtrain[edge_type].edge_index[0].unique() == gval[edge_type].edge_index[0].unique()).all()\n",
    "\n",
    "        folds.append((gtrain, gval))\n",
    "\n",
    "    return folds\n",
    "\n",
    "graph_folds = graph_k_fold(data, datasetConfig.num_folds)\n",
    "graph_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 14532], edge_label=[21798], edge_label_index=[2, 14532], node_type=[2320], edge_type=[14532])\n",
      "[ 0.  1. nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 1, 1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying to make sense of all of this\n",
    "\n",
    "th = tr.to_homogeneous()\n",
    "print(th)\n",
    "print(np.unique(th.edge_label))\n",
    "pos = th.edge_label_index[:, th.edge_label[:14532] == 1]\n",
    "assert (pos == th.edge_index[:, th.edge_type==0]).all()\n",
    "pos.size(), th.edge_index.size()\n",
    "th.node_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  30,   33,   38,  ...,   98,   99,   88],\n",
       "        [ 571, 1771, 1416,  ..., 1111,  150, 2189]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ensure_homogeneous(*args):\n",
    "    def _apply(g):\n",
    "        if isinstance(g, HeteroData):\n",
    "            hg = g.to_homogeneous()\n",
    "            # Removing final na\n",
    "            if hasattr(hg, 'edge_label'):\n",
    "                assert hg.edge_label[hg.edge_label_index.size(1):].isnan().all()\n",
    "                hg.edge_label = hg.edge_label[:hg.edge_label_index.size(1)].bool()\n",
    "            return hg\n",
    "        else:\n",
    "            return g\n",
    "\n",
    "    ret = tuple(_apply(g) for g in args)\n",
    "    if len(ret) == 1:\n",
    "        return ret[0]\n",
    "    else:\n",
    "        return ret\n",
    "\n",
    "_aux = ensure_homogeneous(val)\n",
    "_aux.edge_label_index[:, _aux.edge_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the LightGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 11:40:06,560\tWARNING session.py:100 -- In neither tune session nor train session!\n",
      "/home/daviddavo/.local/lib/python3.10/site-packages/ray/air/session.py:28: UserWarning: `get_checkpoint` is meant to only be called inside a function that is executed by a Tuner or Trainer. Returning `None`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98921fc467944e93aa4eb61509f5aba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " 96%|#########6| 401/416 [00:01<00:00, 398.50it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 11:40:08,667\tWARNING session.py:100 -- In neither tune session nor train session!\n",
      "/home/daviddavo/.local/lib/python3.10/site-packages/ray/air/session.py:28: UserWarning: `report` is meant to only be called inside a function that is executed by a Tuner or Trainer. Returning `None`.\n",
      "  warnings.warn(\n",
      "2023-08-11 11:40:09,640\tWARNING session.py:100 -- In neither tune session nor train session!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.nn import LightGCN\n",
    "\n",
    "# Based on:\n",
    "# - https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "# - https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html\n",
    "# - https://github.com/pyg-team/pytorch_geometric/blob/master/examples/lightgcn.py\n",
    "def train_daostack(train: HeteroData, validation: HeteroData, original: HeteroData, modelConfig: ModelConfig, disable_tqdm=False):\n",
    "    if not isinstance(modelConfig, ModelConfig):\n",
    "        modelConfig = ModelConfig(**modelConfig)\n",
    "    \n",
    "    model = LightGCN(\n",
    "        num_nodes=data.num_nodes,\n",
    "        embedding_dim=modelConfig.embedding_dim,\n",
    "        num_layers=modelConfig.conv_layers,\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=modelConfig.learning_rate)\n",
    "\n",
    "    checkpoint = session.get_checkpoint()\n",
    "\n",
    "    if checkpoint:\n",
    "        checkpoint_state = checkpoint.to_dict()\n",
    "        start_epoch = checkpoint_state[\"epoch\"]\n",
    "        model.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    # Use all message passing edges as training labels\n",
    "    assert train.is_undirected()\n",
    "    assert validation.is_undirected()\n",
    "\n",
    "    # train, validation, test = ensure_homogeneous(train, validation, test)\n",
    "    n_users = train['voter'].num_nodes\n",
    "    n_items = train['proposal'].num_nodes\n",
    "    users = torch.arange(0, n_users, device=device)\n",
    "    items = torch.arange(n_users, n_items+n_users, device=device)\n",
    "    # In message passing, bidirectional edges may cause duplicate information to\n",
    "    # be passed between nodes.\n",
    "    # The official LightGCN usage also uses this line of code (well, for homo graphs)\n",
    "    # - https://github.com/pyg-team/pytorch_geometric/blob/master/examples/lightgcn.py\n",
    "    \n",
    "    \n",
    "    # train_edge_label_index = train.edge_index[:, train.edge_type == 0]\n",
    "    # train.edge_label = train.edge_label[:train.edge_label_index.size(1)] # Now this is done inside ensure_homogeneous\n",
    "    pos_edge_label_index = train['voter', 'votes', 'proposal'].edge_index\n",
    "    neg_edge_label_index = train['voter', 'votes', 'proposal'].negative_samples\n",
    "\n",
    "    # TODO: Use LinkLoader instead (i don't know how)\n",
    "    # Waiting for pyg-team/pytorch_geometric#7817\n",
    "    # train_loader = PyG.loader.LinkLoader(\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        range(pos_edge_label_index.size(1)), # dataset\n",
    "        batch_size=modelConfig.batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _prec_rec(k: int, remove_training=False):\n",
    "        # gt: ground truth (all edges)\n",
    "        gt_index = original['voter', 'votes', 'proposal'].edge_index\n",
    "        if remove_training:\n",
    "            edge_index = validation['voter', 'votes', 'proposal'].edge_index\n",
    "        else:\n",
    "            # All edges\n",
    "            edge_index = original['voter', 'votes', 'proposal'].edge_index\n",
    "\n",
    "        R = item_count = PyG.utils.degree(gt_index[0], num_nodes=n_users)\n",
    "        topr = model.recommend(edge_index, src_index=users, dst_index=items, k=int(R.max()))\n",
    "        \n",
    "        # assert (model.recommend(edge_index, src_index=users, dst_index=items, k=k) == topk).all()\n",
    "        n_samples = len(users)\n",
    "\n",
    "        # [104, 2216]\n",
    "        ground_truth = torch.full((n_users, n_items), False, dtype=torch.bool, device=device)\n",
    "        ground_truth[gt_index[0], gt_index[1] - n_users] = True\n",
    "\n",
    "        isin_rmat = ground_truth.gather(1, topr - n_users)\n",
    "        isin_mat = isin_rmat[:, :k]\n",
    "\n",
    "        prec = (isin_mat.sum(dim=-1) / k).sum() / n_samples\n",
    "        rec = (isin_mat.sum(dim=-1) / item_count).sum() / n_samples\n",
    "\n",
    "        # Now mask isin_rmat to get only up to :R elements\n",
    "        msk = torch.arange(1, R.max()+1, device=device) > R.unsqueeze(1)\n",
    "        isin_rmat[msk] = 0\n",
    "        rprec = (isin_rmat.sum(dim=-1) / R).sum() / n_samples\n",
    "\n",
    "        # print('prec, rec:', (prec, rec))\n",
    "        \n",
    "        return float(prec), float(rec), float(rprec)\n",
    "\n",
    "    for epoch in trange(start_epoch, modelConfig.max_epochs, disable=disable_tqdm):\n",
    "        # index is an array of batch_size that indicates which edges from \n",
    "        # train.edge_index we should use\n",
    "        acc_loss = n_samples = 0\n",
    "        for index in tqdm(train_loader, leave=False, delay=1, disable=disable_tqdm):\n",
    "            pos_edge_index = pos_edge_label_index[:, index]\n",
    "            # neg_edge_index = torch.stack([\n",
    "            #     pos_edge_index[0],\n",
    "            #     # TODO: Use generated negative samples instead\n",
    "            #     torch.randint(n_users, n_users+n_items, index.size(),device=device),\n",
    "            # ])\n",
    "            neg_edge_index = neg_edge_label_index[:, index]\n",
    "            edge_label_index = torch.cat([\n",
    "                pos_edge_index,\n",
    "                neg_edge_index,\n",
    "            ], dim=1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pos_rank, neg_rank = model(train['voter', 'votes', 'proposal'].edge_index, edge_label_index).chunk(2)\n",
    "\n",
    "            # Learning\n",
    "            loss = model.recommendation_loss(\n",
    "                pos_rank,\n",
    "                neg_rank,\n",
    "                node_id=edge_label_index.unique(),\n",
    "                lambda_reg=modelConfig.l2,\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            acc_loss += float(loss) * pos_rank.numel()\n",
    "            n_samples += pos_rank.numel()\n",
    "\n",
    "        checkpoint = Checkpoint.from_dict({\n",
    "            'epoch': epoch,\n",
    "            'net_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        })\n",
    "\n",
    "        # Todo: Add val accuracy (pr@5, rec@5, etc.)\n",
    "        prec5, rec5, rprec = _prec_rec(5, remove_training=False)\n",
    "        prec5t, rec5t, rprect = _prec_rec(5, remove_training=True)\n",
    "        session.report({\n",
    "            'loss': acc_loss/n_samples,\n",
    "            'rprec train': rprec, 'rprec test': rprect,\n",
    "            'p@5 train': prec5, 'p@5 test': prec5t,\n",
    "            'r@5 train': rec5, 'r@5 test': rec5t,\n",
    "        }, checkpoint=checkpoint)\n",
    "\n",
    "# Testing just syntax errors\n",
    "train_daostack(graph_folds[0][0].to(device), graph_folds[0][1].to(device), data.to(device), ModelConfig(**(modelConfig._asdict() | {'max_epochs':2}))),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-08-11 11:40:40</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:28.66        </td></tr>\n",
       "<tr><td>Memory:      </td><td>14.5/125.6 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 1.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 17<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                                                          </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>_aux_train_daostack_3b83494c</td><td style=\"text-align: right;\">           1</td><td>/home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_3b83494c_1_trial_index=0,batch_size=7,conv_layers=2,embedding_dim=55,l2=0.0000,learning_rate=0.0084,max_epochs_2023-08-11_11-40-12/error.txt</td></tr>\n",
       "<tr><td>_aux_train_daostack_7dada229</td><td style=\"text-align: right;\">           1</td><td>/home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_7dada229_2_trial_index=1,batch_size=7,conv_layers=2,embedding_dim=55,l2=0.0000,learning_rate=0.0084,max_epochs_2023-08-11_11-40-13/error.txt</td></tr>\n",
       "<tr><td>_aux_train_daostack_00eddffc</td><td style=\"text-align: right;\">           1</td><td>/home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_00eddffc_3_trial_index=2,batch_size=7,conv_layers=2,embedding_dim=55,l2=0.0000,learning_rate=0.0084,max_epochs_2023-08-11_11-40-15/error.txt</td></tr>\n",
       "<tr><td>_aux_train_daostack_8ea19386</td><td style=\"text-align: right;\">           1</td><td>/home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_8ea19386_4_trial_index=3,batch_size=7,conv_layers=2,embedding_dim=55,l2=0.0000,learning_rate=0.0084,max_epochs_2023-08-11_11-40-17/error.txt</td></tr>\n",
       "<tr><td>_aux_train_daostack_c3b4de2c</td><td style=\"text-align: right;\">           1</td><td>/home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_c3b4de2c_5_trial_index=4,batch_size=7,conv_layers=2,embedding_dim=55,l2=0.0000,learning_rate=0.0084,max_epochs_2023-08-11_11-40-18/error.txt</td></tr>\n",
       "<tr><td>_aux_train_daostack_c6d1b9ac</td><td style=\"text-align: right;\">           1</td><td>/home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_c6d1b9ac_6_trial_index=0,batch_size=6,conv_layers=2,embedding_dim=155,l2=0.0000,learning_rate=0.0165,max_epoch_2023-08-11_11-40-20/error.txt</td></tr>\n",
       "<tr><td>_aux_train_daostack_a54411cb</td><td style=\"text-align: right;\">           1</td><td>/home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_a54411cb_7_trial_index=1,batch_size=6,conv_layers=2,embedding_dim=155,l2=0.0000,learning_rate=0.0165,max_epoch_2023-08-11_11-40-21/error.txt</td></tr>\n",
       "<tr><td>_aux_train_daostack_9aa2e998</td><td style=\"text-align: right;\">           1</td><td>/home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_9aa2e998_8_trial_index=2,batch_size=6,conv_layers=2,embedding_dim=155,l2=0.0000,learning_rate=0.0165,max_epoch_2023-08-11_11-40-23/error.txt</td></tr>\n",
       "<tr><td>_aux_train_daostack_efa240ee</td><td style=\"text-align: right;\">           1</td><td>/home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_efa240ee_9_trial_index=3,batch_size=6,conv_layers=2,embedding_dim=155,l2=0.0000,learning_rate=0.0165,max_epoch_2023-08-11_11-40-25/error.txt</td></tr>\n",
       "<tr><td>_aux_train_daostack_d35497f1</td><td style=\"text-align: right;\">           1</td><td>/home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_d35497f1_10_trial_index=4,batch_size=6,conv_layers=2,embedding_dim=155,l2=0.0000,learning_rate=0.0165,max_epoc_2023-08-11_11-40-26/error.txt</td></tr>\n",
       "<tr><td>_aux_train_daostack_ffc43055</td><td style=\"text-align: right;\">           1</td><td>/home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_ffc43055_11_trial_index=0,batch_size=8,conv_layers=3,embedding_dim=15,l2=0.0000,learning_rate=0.0012,max_epoch_2023-08-11_11-40-28/error.txt</td></tr>\n",
       "<tr><td>_aux_train_daostack_83732aca</td><td style=\"text-align: right;\">           1</td><td>/home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_83732aca_12_trial_index=1,batch_size=8,conv_layers=3,embedding_dim=15,l2=0.0000,learning_rate=0.0012,max_epoch_2023-08-11_11-40-30/error.txt</td></tr>\n",
       "<tr><td>_aux_train_daostack_825b6f5e</td><td style=\"text-align: right;\">           1</td><td>/home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_825b6f5e_13_trial_index=2,batch_size=8,conv_layers=3,embedding_dim=15,l2=0.0000,learning_rate=0.0012,max_epoch_2023-08-11_11-40-31/error.txt</td></tr>\n",
       "<tr><td>_aux_train_daostack_5eaa0c6f</td><td style=\"text-align: right;\">           1</td><td>/home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_5eaa0c6f_14_trial_index=3,batch_size=8,conv_layers=3,embedding_dim=15,l2=0.0000,learning_rate=0.0012,max_epoch_2023-08-11_11-40-33/error.txt</td></tr>\n",
       "<tr><td>_aux_train_daostack_312fad16</td><td style=\"text-align: right;\">           1</td><td>/home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_312fad16_15_trial_index=4,batch_size=8,conv_layers=3,embedding_dim=15,l2=0.0000,learning_rate=0.0012,max_epoch_2023-08-11_11-40-34/error.txt</td></tr>\n",
       "<tr><td>_aux_train_daostack_ccacf83d</td><td style=\"text-align: right;\">           1</td><td>/home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_ccacf83d_16_trial_index=0,batch_size=8,conv_layers=5,embedding_dim=205,l2=0.0014,learning_rate=0.0004,max_epoc_2023-08-11_11-40-36/error.txt</td></tr>\n",
       "<tr><td>_aux_train_daostack_a0e8d0ee</td><td style=\"text-align: right;\">           1</td><td>/home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_a0e8d0ee_17_trial_index=1,batch_size=8,conv_layers=5,embedding_dim=205,l2=0.0014,learning_rate=0.0004,max_epoc_2023-08-11_11-40-38/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  __trial_index__</th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  conv_layers</th><th style=\"text-align: right;\">  embedding_dim</th><th style=\"text-align: right;\">         l2</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  max_epochs</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>_aux_train_daostack_8e909f75</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">            5</td><td style=\"text-align: right;\">            205</td><td style=\"text-align: right;\">0.00137509 </td><td style=\"text-align: right;\">        0.00044</td><td style=\"text-align: right;\">          50</td></tr>\n",
       "<tr><td>_aux_train_daostack_3b83494c</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">           7</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">             55</td><td style=\"text-align: right;\">1.33085e-06</td><td style=\"text-align: right;\">        0.00841</td><td style=\"text-align: right;\">          50</td></tr>\n",
       "<tr><td>_aux_train_daostack_7dada229</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           7</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">             55</td><td style=\"text-align: right;\">1.33085e-06</td><td style=\"text-align: right;\">        0.00841</td><td style=\"text-align: right;\">          50</td></tr>\n",
       "<tr><td>_aux_train_daostack_00eddffc</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">           7</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">             55</td><td style=\"text-align: right;\">1.33085e-06</td><td style=\"text-align: right;\">        0.00841</td><td style=\"text-align: right;\">          50</td></tr>\n",
       "<tr><td>_aux_train_daostack_8ea19386</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">           7</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">             55</td><td style=\"text-align: right;\">1.33085e-06</td><td style=\"text-align: right;\">        0.00841</td><td style=\"text-align: right;\">          50</td></tr>\n",
       "<tr><td>_aux_train_daostack_c3b4de2c</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">                4</td><td style=\"text-align: right;\">           7</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">             55</td><td style=\"text-align: right;\">1.33085e-06</td><td style=\"text-align: right;\">        0.00841</td><td style=\"text-align: right;\">          50</td></tr>\n",
       "<tr><td>_aux_train_daostack_c6d1b9ac</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">           6</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">            155</td><td style=\"text-align: right;\">3.04021e-09</td><td style=\"text-align: right;\">        0.01653</td><td style=\"text-align: right;\">          50</td></tr>\n",
       "<tr><td>_aux_train_daostack_a54411cb</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           6</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">            155</td><td style=\"text-align: right;\">3.04021e-09</td><td style=\"text-align: right;\">        0.01653</td><td style=\"text-align: right;\">          50</td></tr>\n",
       "<tr><td>_aux_train_daostack_9aa2e998</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">           6</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">            155</td><td style=\"text-align: right;\">3.04021e-09</td><td style=\"text-align: right;\">        0.01653</td><td style=\"text-align: right;\">          50</td></tr>\n",
       "<tr><td>_aux_train_daostack_efa240ee</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">           6</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">            155</td><td style=\"text-align: right;\">3.04021e-09</td><td style=\"text-align: right;\">        0.01653</td><td style=\"text-align: right;\">          50</td></tr>\n",
       "<tr><td>_aux_train_daostack_d35497f1</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">                4</td><td style=\"text-align: right;\">           6</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">            155</td><td style=\"text-align: right;\">3.04021e-09</td><td style=\"text-align: right;\">        0.01653</td><td style=\"text-align: right;\">          50</td></tr>\n",
       "<tr><td>_aux_train_daostack_ffc43055</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">             15</td><td style=\"text-align: right;\">4.54786e-08</td><td style=\"text-align: right;\">        0.00117</td><td style=\"text-align: right;\">          50</td></tr>\n",
       "<tr><td>_aux_train_daostack_83732aca</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">             15</td><td style=\"text-align: right;\">4.54786e-08</td><td style=\"text-align: right;\">        0.00117</td><td style=\"text-align: right;\">          50</td></tr>\n",
       "<tr><td>_aux_train_daostack_825b6f5e</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">             15</td><td style=\"text-align: right;\">4.54786e-08</td><td style=\"text-align: right;\">        0.00117</td><td style=\"text-align: right;\">          50</td></tr>\n",
       "<tr><td>_aux_train_daostack_5eaa0c6f</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">             15</td><td style=\"text-align: right;\">4.54786e-08</td><td style=\"text-align: right;\">        0.00117</td><td style=\"text-align: right;\">          50</td></tr>\n",
       "<tr><td>_aux_train_daostack_312fad16</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">                4</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">             15</td><td style=\"text-align: right;\">4.54786e-08</td><td style=\"text-align: right;\">        0.00117</td><td style=\"text-align: right;\">          50</td></tr>\n",
       "<tr><td>_aux_train_daostack_ccacf83d</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">            5</td><td style=\"text-align: right;\">            205</td><td style=\"text-align: right;\">0.00137509 </td><td style=\"text-align: right;\">        0.00044</td><td style=\"text-align: right;\">          50</td></tr>\n",
       "<tr><td>_aux_train_daostack_a0e8d0ee</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">            5</td><td style=\"text-align: right;\">            205</td><td style=\"text-align: right;\">0.00137509 </td><td style=\"text-align: right;\">        0.00044</td><td style=\"text-align: right;\">          50</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 11:40:13,840\tWARNING worker.py:2006 -- Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:13,845\tWARNING worker.py:2006 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffe424132c238609b96a6d9f0001000000 Worker ID: 8173792df1e46dcd395a1ce7508adba7c09677df916d08eec47ce805 Node ID: ee2880c14db99f7946fa0fd5392791b0fe33f6da29f48f8abb86a603 Worker IP address: 147.96.81.131 Worker port: 39761 Worker PID: 246036 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:13,846\tERROR tune_controller.py:911 -- Trial task failed for trial _aux_train_daostack_3b83494c\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2495, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ImplicitFunc\n",
      "\tactor_id: e424132c238609b96a6d9f0001000000\n",
      "\tpid: 246036\n",
      "\tnamespace: 9f8689b7-3125-4320-8d24-93b0086ccea6\n",
      "\tip: 147.96.81.131\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m Worker exits with an exit code None.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m  Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     raise RuntimeError(\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     actor_class = pickle.loads(pickled_class)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     return torch.load(io.BytesIO(b))\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     result = unpickler.load()\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     wrap_storage=restore_location(obj, location),\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     result = fn(storage, location)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m RuntimeError: CUDA error: out of memory\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     raise RuntimeError(\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     actor_class = pickle.loads(pickled_class)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     return torch.load(io.BytesIO(b))\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     result = unpickler.load()\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     wrap_storage=restore_location(obj, location),\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     result = fn(storage, location)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m RuntimeError: CUDA error: out of memory\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m An unexpected internal error occurred while the worker was executing a task.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     raise RuntimeError(\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     actor_class = pickle.loads(pickled_class)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     return torch.load(io.BytesIO(b))\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     result = unpickler.load()\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     wrap_storage=restore_location(obj, location),\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     result = fn(storage, location)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m RuntimeError: CUDA error: out of memory\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     raise RuntimeError(\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     actor_class = pickle.loads(pickled_class)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     return torch.load(io.BytesIO(b))\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     result = unpickler.load()\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     wrap_storage=restore_location(obj, location),\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     result = fn(storage, location)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m     return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m RuntimeError: CUDA error: out of memory\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1830, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246036)\u001b[0m SystemExit\n",
      "2023-08-11 11:40:15,432\tWARNING worker.py:2006 -- Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:15,437\tWARNING worker.py:2006 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffb2ec7d4aa67eabbf3f53130601000000 Worker ID: c2893989239f3ba5297efc8825bc9757e8a15aeff8b147bf094ea65e Node ID: ee2880c14db99f7946fa0fd5392791b0fe33f6da29f48f8abb86a603 Worker IP address: 147.96.81.131 Worker port: 35857 Worker PID: 246089 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:15,438\tERROR tune_controller.py:911 -- Trial task failed for trial _aux_train_daostack_7dada229\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2495, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ImplicitFunc\n",
      "\tactor_id: b2ec7d4aa67eabbf3f53130601000000\n",
      "\tpid: 246089\n",
      "\tnamespace: 9f8689b7-3125-4320-8d24-93b0086ccea6\n",
      "\tip: 147.96.81.131\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:17,055\tWARNING worker.py:2006 -- Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:17,059\tWARNING worker.py:2006 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff4b9416f844b99870255385ca01000000 Worker ID: f71def8f62022067f9b475232f784c9c2b76350035fbd048c30934da Node ID: ee2880c14db99f7946fa0fd5392791b0fe33f6da29f48f8abb86a603 Worker IP address: 147.96.81.131 Worker port: 43207 Worker PID: 246133 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:17,060\tERROR tune_controller.py:911 -- Trial task failed for trial _aux_train_daostack_00eddffc\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2495, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ImplicitFunc\n",
      "\tactor_id: 4b9416f844b99870255385ca01000000\n",
      "\tpid: 246133\n",
      "\tnamespace: 9f8689b7-3125-4320-8d24-93b0086ccea6\n",
      "\tip: 147.96.81.131\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:18,659\tWARNING worker.py:2006 -- Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:18,665\tWARNING worker.py:2006 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff5e4d702cc2d04fb4e64b602d01000000 Worker ID: fe3ea0d6b5118dc64c45288b4d39ad34b6d50e2e36f531e60208f160 Node ID: ee2880c14db99f7946fa0fd5392791b0fe33f6da29f48f8abb86a603 Worker IP address: 147.96.81.131 Worker port: 44785 Worker PID: 246178 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:18,666\tERROR tune_controller.py:911 -- Trial task failed for trial _aux_train_daostack_8ea19386\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2495, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ImplicitFunc\n",
      "\tactor_id: 5e4d702cc2d04fb4e64b602d01000000\n",
      "\tpid: 246178\n",
      "\tnamespace: 9f8689b7-3125-4320-8d24-93b0086ccea6\n",
      "\tip: 147.96.81.131\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:20,299\tWARNING worker.py:2006 -- Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:20,305\tWARNING worker.py:2006 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: fffffffffffffffff22be60045e7c23c8ae649ae01000000 Worker ID: 6acc1c040bea3fa6e9c07c12e6e50c8b4f4d1f82cf4def97178bb2fc Node ID: ee2880c14db99f7946fa0fd5392791b0fe33f6da29f48f8abb86a603 Worker IP address: 147.96.81.131 Worker port: 43987 Worker PID: 246220 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:20,306\tERROR tune_controller.py:911 -- Trial task failed for trial _aux_train_daostack_c3b4de2c\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2495, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ImplicitFunc\n",
      "\tactor_id: f22be60045e7c23c8ae649ae01000000\n",
      "\tpid: 246220\n",
      "\tnamespace: 9f8689b7-3125-4320-8d24-93b0086ccea6\n",
      "\tip: 147.96.81.131\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "/home/daviddavo/.local/lib/python3.10/site-packages/ray/tune/search/repeater.py:176: RuntimeWarning: Mean of empty slice\n",
      "  result={self.searcher.metric: np.nanmean(scores)},\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m Worker exits with an exit code None.\u001b[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 36x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\u001b[32m [repeated 48x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m     return method(__ray_actor, *args, **kwargs)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m     raise RuntimeError(\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m \u001b[32m [repeated 72x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m     actor_class = pickle.loads(pickled_class)\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m     return torch.load(io.BytesIO(b))\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m     return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m     result = unpickler.load()\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m     wrap_storage=restore_location(obj, location),\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m     result = fn(storage, location)\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m     return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m RuntimeError: CUDA error: out of memory\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m During handling of the above exception, another exception occurred:\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1830, in ray._raylet.task_execution_handler\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m   File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m An unexpected internal error occurred while the worker was executing a task.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246220)\u001b[0m SystemExit\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2023-08-11 11:40:21,954\tWARNING worker.py:2006 -- Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:21,959\tWARNING worker.py:2006 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff242bf09e7ae5b81d29bdf79101000000 Worker ID: 4483d2412edc33191a11c9392a532ca36b620302ba02b2c4c6895d41 Node ID: ee2880c14db99f7946fa0fd5392791b0fe33f6da29f48f8abb86a603 Worker IP address: 147.96.81.131 Worker port: 36775 Worker PID: 246262 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:21,960\tERROR tune_controller.py:911 -- Trial task failed for trial _aux_train_daostack_c6d1b9ac\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2495, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ImplicitFunc\n",
      "\tactor_id: 242bf09e7ae5b81d29bdf79101000000\n",
      "\tpid: 246262\n",
      "\tnamespace: 9f8689b7-3125-4320-8d24-93b0086ccea6\n",
      "\tip: 147.96.81.131\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:23,598\tWARNING worker.py:2006 -- Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:23,602\tWARNING worker.py:2006 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff8c2832c3a3d68e71acf99b4b01000000 Worker ID: bdab4aa70ad94ad9bc029488aabd71bbddd082fbfcc5ac035d3274c0 Node ID: ee2880c14db99f7946fa0fd5392791b0fe33f6da29f48f8abb86a603 Worker IP address: 147.96.81.131 Worker port: 37365 Worker PID: 246305 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:23,603\tERROR tune_controller.py:911 -- Trial task failed for trial _aux_train_daostack_a54411cb\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2495, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ImplicitFunc\n",
      "\tactor_id: 8c2832c3a3d68e71acf99b4b01000000\n",
      "\tpid: 246305\n",
      "\tnamespace: 9f8689b7-3125-4320-8d24-93b0086ccea6\n",
      "\tip: 147.96.81.131\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:25,218\tWARNING worker.py:2006 -- Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:25,222\tWARNING worker.py:2006 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff17ba9ef4a9891491205b9f7d01000000 Worker ID: e70f3645228bee5028611d48c1f8ecee3bea843d3da12b24656c9b8f Node ID: ee2880c14db99f7946fa0fd5392791b0fe33f6da29f48f8abb86a603 Worker IP address: 147.96.81.131 Worker port: 43107 Worker PID: 246347 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:25,223\tERROR tune_controller.py:911 -- Trial task failed for trial _aux_train_daostack_9aa2e998\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2495, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ImplicitFunc\n",
      "\tactor_id: 17ba9ef4a9891491205b9f7d01000000\n",
      "\tpid: 246347\n",
      "\tnamespace: 9f8689b7-3125-4320-8d24-93b0086ccea6\n",
      "\tip: 147.96.81.131\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:26,830\tWARNING worker.py:2006 -- Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:26,834\tWARNING worker.py:2006 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff8c82e77e36b043240205eab101000000 Worker ID: cd8d6a3a4a8e1f555726a09b06f052e15a069f0a66752889a0f8e196 Node ID: ee2880c14db99f7946fa0fd5392791b0fe33f6da29f48f8abb86a603 Worker IP address: 147.96.81.131 Worker port: 32931 Worker PID: 246389 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:26,835\tERROR tune_controller.py:911 -- Trial task failed for trial _aux_train_daostack_efa240ee\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2495, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ImplicitFunc\n",
      "\tactor_id: 8c82e77e36b043240205eab101000000\n",
      "\tpid: 246389\n",
      "\tnamespace: 9f8689b7-3125-4320-8d24-93b0086ccea6\n",
      "\tip: 147.96.81.131\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m Worker exits with an exit code None.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 36x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\u001b[32m [repeated 48x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m     return method(__ray_actor, *args, **kwargs)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m     raise RuntimeError(\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m \u001b[32m [repeated 72x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m     actor_class = pickle.loads(pickled_class)\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m     return torch.load(io.BytesIO(b))\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m     return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m     result = unpickler.load()\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m     wrap_storage=restore_location(obj, location),\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m     result = fn(storage, location)\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m     return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m RuntimeError: CUDA error: out of memory\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m During handling of the above exception, another exception occurred:\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1830, in ray._raylet.task_execution_handler\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m   File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m An unexpected internal error occurred while the worker was executing a task.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246389)\u001b[0m SystemExit\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2023-08-11 11:40:28,432\tWARNING worker.py:2006 -- Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:28,436\tWARNING worker.py:2006 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffe6385b2b70728c70b131e7f201000000 Worker ID: c56d5823415a76291156a63a309ea6bb0dbb6efe0e5dd4042bdf4d70 Node ID: ee2880c14db99f7946fa0fd5392791b0fe33f6da29f48f8abb86a603 Worker IP address: 147.96.81.131 Worker port: 44097 Worker PID: 246431 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:28,437\tERROR tune_controller.py:911 -- Trial task failed for trial _aux_train_daostack_d35497f1\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2495, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ImplicitFunc\n",
      "\tactor_id: e6385b2b70728c70b131e7f201000000\n",
      "\tpid: 246431\n",
      "\tnamespace: 9f8689b7-3125-4320-8d24-93b0086ccea6\n",
      "\tip: 147.96.81.131\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:30,017\tWARNING worker.py:2006 -- Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:30,021\tWARNING worker.py:2006 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff1ada8b76813eb7b96936de7701000000 Worker ID: 423541e3d62c7f14ba7f83e098af6ba55a46549fcc60dd0a7db8e934 Node ID: ee2880c14db99f7946fa0fd5392791b0fe33f6da29f48f8abb86a603 Worker IP address: 147.96.81.131 Worker port: 33669 Worker PID: 246473 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:30,022\tERROR tune_controller.py:911 -- Trial task failed for trial _aux_train_daostack_ffc43055\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2495, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ImplicitFunc\n",
      "\tactor_id: 1ada8b76813eb7b96936de7701000000\n",
      "\tpid: 246473\n",
      "\tnamespace: 9f8689b7-3125-4320-8d24-93b0086ccea6\n",
      "\tip: 147.96.81.131\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:31,628\tWARNING worker.py:2006 -- Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:31,631\tWARNING worker.py:2006 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffa49c437595b8fa247fa05caf01000000 Worker ID: de35aec689c5b657e03a7ec09c3dc3e8f3a1e495b32269a55f3e794f Node ID: ee2880c14db99f7946fa0fd5392791b0fe33f6da29f48f8abb86a603 Worker IP address: 147.96.81.131 Worker port: 33405 Worker PID: 246515 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:31,633\tERROR tune_controller.py:911 -- Trial task failed for trial _aux_train_daostack_83732aca\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2495, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ImplicitFunc\n",
      "\tactor_id: a49c437595b8fa247fa05caf01000000\n",
      "\tpid: 246515\n",
      "\tnamespace: 9f8689b7-3125-4320-8d24-93b0086ccea6\n",
      "\tip: 147.96.81.131\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:33,235\tWARNING worker.py:2006 -- Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:33,241\tWARNING worker.py:2006 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff1228641821d729e46b5f9da501000000 Worker ID: a430dbce9347f98c98647eebf2c3b23244ed028e1ddca30a0f62d0d2 Node ID: ee2880c14db99f7946fa0fd5392791b0fe33f6da29f48f8abb86a603 Worker IP address: 147.96.81.131 Worker port: 44701 Worker PID: 246558 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:33,242\tERROR tune_controller.py:911 -- Trial task failed for trial _aux_train_daostack_825b6f5e\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2495, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ImplicitFunc\n",
      "\tactor_id: 1228641821d729e46b5f9da501000000\n",
      "\tpid: 246558\n",
      "\tnamespace: 9f8689b7-3125-4320-8d24-93b0086ccea6\n",
      "\tip: 147.96.81.131\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m Worker exits with an exit code None.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 36x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\u001b[32m [repeated 48x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m     return method(__ray_actor, *args, **kwargs)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m     raise RuntimeError(\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m \u001b[32m [repeated 72x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m     actor_class = pickle.loads(pickled_class)\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m     return torch.load(io.BytesIO(b))\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m     return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m     result = unpickler.load()\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m     wrap_storage=restore_location(obj, location),\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m     result = fn(storage, location)\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m     return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m RuntimeError: CUDA error: out of memory\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m During handling of the above exception, another exception occurred:\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1830, in ray._raylet.task_execution_handler\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m   File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m An unexpected internal error occurred while the worker was executing a task.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246558)\u001b[0m SystemExit\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2023-08-11 11:40:34,827\tWARNING worker.py:2006 -- Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:34,832\tWARNING worker.py:2006 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff48415401e87ffb782001ed3501000000 Worker ID: f08d448a331f24e543a4b3942b3bacced6fd4b3fb552653b6645d555 Node ID: ee2880c14db99f7946fa0fd5392791b0fe33f6da29f48f8abb86a603 Worker IP address: 147.96.81.131 Worker port: 34631 Worker PID: 246600 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:34,833\tERROR tune_controller.py:911 -- Trial task failed for trial _aux_train_daostack_5eaa0c6f\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2495, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ImplicitFunc\n",
      "\tactor_id: 48415401e87ffb782001ed3501000000\n",
      "\tpid: 246600\n",
      "\tnamespace: 9f8689b7-3125-4320-8d24-93b0086ccea6\n",
      "\tip: 147.96.81.131\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:36,433\tWARNING worker.py:2006 -- Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:36,437\tWARNING worker.py:2006 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffc7067dc7f69a23d0cf4fb8e101000000 Worker ID: 1e3eb920bf0cc0eead4fdf314209856b4ba89b5335f6d6a50cef3f9d Node ID: ee2880c14db99f7946fa0fd5392791b0fe33f6da29f48f8abb86a603 Worker IP address: 147.96.81.131 Worker port: 37369 Worker PID: 246642 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:36,438\tERROR tune_controller.py:911 -- Trial task failed for trial _aux_train_daostack_312fad16\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2495, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ImplicitFunc\n",
      "\tactor_id: c7067dc7f69a23d0cf4fb8e101000000\n",
      "\tpid: 246642\n",
      "\tnamespace: 9f8689b7-3125-4320-8d24-93b0086ccea6\n",
      "\tip: 147.96.81.131\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:38,030\tWARNING worker.py:2006 -- Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:38,034\tWARNING worker.py:2006 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff372033d33219bfc6a8d1501201000000 Worker ID: abe6eb065d4131311a4708817796fa458f22ed9d76f4d66770a1bc75 Node ID: ee2880c14db99f7946fa0fd5392791b0fe33f6da29f48f8abb86a603 Worker IP address: 147.96.81.131 Worker port: 40303 Worker PID: 246684 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:38,035\tERROR tune_controller.py:911 -- Trial task failed for trial _aux_train_daostack_ccacf83d\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2495, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ImplicitFunc\n",
      "\tactor_id: 372033d33219bfc6a8d1501201000000\n",
      "\tpid: 246684\n",
      "\tnamespace: 9f8689b7-3125-4320-8d24-93b0086ccea6\n",
      "\tip: 147.96.81.131\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:39,623\tWARNING worker.py:2006 -- Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:39,627\tWARNING worker.py:2006 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff95b2fe831315964ca0a8724b01000000 Worker ID: 2e995a2413e982453ddf6de7e40a7caf968849d135badaeccd212f2d Node ID: ee2880c14db99f7946fa0fd5392791b0fe33f6da29f48f8abb86a603 Worker IP address: 147.96.81.131 Worker port: 43541 Worker PID: 246726 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:39,628\tERROR tune_controller.py:911 -- Trial task failed for trial _aux_train_daostack_a0e8d0ee\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2495, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ImplicitFunc\n",
      "\tactor_id: 95b2fe831315964ca0a8724b01000000\n",
      "\tpid: 246726\n",
      "\tnamespace: 9f8689b7-3125-4320-8d24-93b0086ccea6\n",
      "\tip: 147.96.81.131\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m Worker exits with an exit code None.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 36x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\u001b[32m [repeated 48x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m     return method(__ray_actor, *args, **kwargs)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m     raise RuntimeError(\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m \u001b[32m [repeated 72x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m     actor_class = pickle.loads(pickled_class)\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m     return torch.load(io.BytesIO(b))\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m     return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m     result = unpickler.load()\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m     wrap_storage=restore_location(obj, location),\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m     result = fn(storage, location)\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m     return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m RuntimeError: CUDA error: out of memory\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m During handling of the above exception, another exception occurred:\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1830, in ray._raylet.task_execution_handler\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m   File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m An unexpected internal error occurred while the worker was executing a task.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246726)\u001b[0m SystemExit\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2023-08-11 11:40:40,850\tWARNING tune.py:192 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2023-08-11 11:40:41,206\tWARNING worker.py:2006 -- Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:41,210\tWARNING worker.py:2006 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff77fc7bcc9a32bac99f7d6eb201000000 Worker ID: f53946fcfe067bcff65ea47afd7ac342db995cc3f9a5e4536262868a Node ID: ee2880c14db99f7946fa0fd5392791b0fe33f6da29f48f8abb86a603 Worker IP address: 147.96.81.131 Worker port: 43921 Worker PID: 246768 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n",
      " Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2023-08-11 11:40:41,211\tERROR tune.py:1144 -- Trials did not complete: [_aux_train_daostack_3b83494c, _aux_train_daostack_7dada229, _aux_train_daostack_00eddffc, _aux_train_daostack_8ea19386, _aux_train_daostack_c3b4de2c, _aux_train_daostack_c6d1b9ac, _aux_train_daostack_a54411cb, _aux_train_daostack_9aa2e998, _aux_train_daostack_efa240ee, _aux_train_daostack_d35497f1, _aux_train_daostack_ffc43055, _aux_train_daostack_83732aca, _aux_train_daostack_825b6f5e, _aux_train_daostack_5eaa0c6f, _aux_train_daostack_312fad16, _aux_train_daostack_ccacf83d, _aux_train_daostack_a0e8d0ee]\n",
      "2023-08-11 11:40:41,211\tINFO tune.py:1148 -- Total run time: 29.05 seconds (28.66 seconds for the tuning loop).\n",
      "2023-08-11 11:40:41,211\tWARNING tune.py:1163 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: Tuner.restore(path=\"/home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09\", trainable=...)\n",
      "2023-08-11 11:40:41,214\tWARNING experiment_analysis.py:916 -- Failed to read the results for 18 trials:\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_3b83494c_1_trial_index=0,batch_size=7,conv_layers=2,embedding_dim=55,l2=0.0000,learning_rate=0.0084,max_epochs_2023-08-11_11-40-12\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_7dada229_2_trial_index=1,batch_size=7,conv_layers=2,embedding_dim=55,l2=0.0000,learning_rate=0.0084,max_epochs_2023-08-11_11-40-13\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_00eddffc_3_trial_index=2,batch_size=7,conv_layers=2,embedding_dim=55,l2=0.0000,learning_rate=0.0084,max_epochs_2023-08-11_11-40-15\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_8ea19386_4_trial_index=3,batch_size=7,conv_layers=2,embedding_dim=55,l2=0.0000,learning_rate=0.0084,max_epochs_2023-08-11_11-40-17\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_c3b4de2c_5_trial_index=4,batch_size=7,conv_layers=2,embedding_dim=55,l2=0.0000,learning_rate=0.0084,max_epochs_2023-08-11_11-40-18\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_c6d1b9ac_6_trial_index=0,batch_size=6,conv_layers=2,embedding_dim=155,l2=0.0000,learning_rate=0.0165,max_epoch_2023-08-11_11-40-20\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_a54411cb_7_trial_index=1,batch_size=6,conv_layers=2,embedding_dim=155,l2=0.0000,learning_rate=0.0165,max_epoch_2023-08-11_11-40-21\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_9aa2e998_8_trial_index=2,batch_size=6,conv_layers=2,embedding_dim=155,l2=0.0000,learning_rate=0.0165,max_epoch_2023-08-11_11-40-23\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_efa240ee_9_trial_index=3,batch_size=6,conv_layers=2,embedding_dim=155,l2=0.0000,learning_rate=0.0165,max_epoch_2023-08-11_11-40-25\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_d35497f1_10_trial_index=4,batch_size=6,conv_layers=2,embedding_dim=155,l2=0.0000,learning_rate=0.0165,max_epoc_2023-08-11_11-40-26\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_ffc43055_11_trial_index=0,batch_size=8,conv_layers=3,embedding_dim=15,l2=0.0000,learning_rate=0.0012,max_epoch_2023-08-11_11-40-28\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_83732aca_12_trial_index=1,batch_size=8,conv_layers=3,embedding_dim=15,l2=0.0000,learning_rate=0.0012,max_epoch_2023-08-11_11-40-30\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_825b6f5e_13_trial_index=2,batch_size=8,conv_layers=3,embedding_dim=15,l2=0.0000,learning_rate=0.0012,max_epoch_2023-08-11_11-40-31\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_5eaa0c6f_14_trial_index=3,batch_size=8,conv_layers=3,embedding_dim=15,l2=0.0000,learning_rate=0.0012,max_epoch_2023-08-11_11-40-33\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_312fad16_15_trial_index=4,batch_size=8,conv_layers=3,embedding_dim=15,l2=0.0000,learning_rate=0.0012,max_epoch_2023-08-11_11-40-34\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_ccacf83d_16_trial_index=0,batch_size=8,conv_layers=5,embedding_dim=205,l2=0.0014,learning_rate=0.0004,max_epoc_2023-08-11_11-40-36\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_a0e8d0ee_17_trial_index=1,batch_size=8,conv_layers=5,embedding_dim=205,l2=0.0014,learning_rate=0.0004,max_epoc_2023-08-11_11-40-38\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_8e909f75_18_trial_index=2,batch_size=8,conv_layers=5,embedding_dim=205,l2=0.0014,learning_rate=0.0004,max_epoc_2023-08-11_11-40-39\n"
     ]
    }
   ],
   "source": [
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search import Repeater\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "\n",
    "def _aux_train_daostack(config):\n",
    "    # TODO: Is bad practice to pass a dataset trainable\n",
    "    # config['embedding_dim'] = 2**config['embedding_dim']\n",
    "    config['batch_size'] = 2**config['batch_size']\n",
    "    n_fold = config.pop('__trial_index__')\n",
    "    train, validation = graph_folds[n_fold]\n",
    "    return train_daostack(train.to(device), validation.to(device), data.to(device), config, disable_tqdm=True)\n",
    "\n",
    "tryConfigs = ModelConfig(\n",
    "    max_epochs=50,\n",
    "    conv_layers=tune.randint(2,6),\n",
    "    learning_rate=tune.qloguniform(1e-5, 1, 1e-5),\n",
    "    l2=tune.loguniform(1e-9, 1e-1),\n",
    "    # These will be 2 to the power\n",
    "    batch_size=tune.randint(4,10), # 16..1024\n",
    "    # embedding_dim=tune.randint(4,8), # 16..128\n",
    "    embedding_dim=tune.qlograndint(10, 500, 5),\n",
    ")\n",
    "\n",
    "# It is recommended to not use Repeater with a TrialScheduler. Early termination can negatively affect the average reported metric.\n",
    "asha_scheduler = None\n",
    "# asha_scheduler = ASHAScheduler(\n",
    "#     time_attr='training_iteration',\n",
    "#     max_t=50,\n",
    "#     grace_period=5,\n",
    "#     reduction_factor=3,\n",
    "#     brackets=1,\n",
    "# )\n",
    "\n",
    "search_alg = HyperOptSearch()\n",
    "search_alg = Repeater(search_alg,datasetConfig.num_folds)\n",
    "\n",
    "# Every run takes approx half a gig of vram (no optimizations)\n",
    "# The RTX 4090 has 24GB so we can run the model about 48 times\n",
    "resources_per_trial={\n",
    "    'cpu': 1,\n",
    "    'memory': 0 if torch.cuda.is_available() else 2e9,\n",
    "    # 'gpu': 1/32 if torch.cuda.is_available() else 0,\n",
    "    'gpu': torch.cuda.is_available(),\n",
    "}\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(_aux_train_daostack, resources_per_trial),\n",
    "    param_space=tryConfigs._asdict(),\n",
    "    tune_config=tune.TuneConfig(\n",
    "        # time_budget_s=60,\n",
    "        num_samples=datasetConfig.num_folds*500,\n",
    "        scheduler=asha_scheduler,\n",
    "        search_alg=search_alg,\n",
    "        metric='rprec test',\n",
    "        mode='max',\n",
    "    )\n",
    ")\n",
    "exp = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 11:40:41,218\tWARNING experiment_analysis.py:694 -- Failed to read the config for 18 trials:\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_3b83494c_1_trial_index=0,batch_size=7,conv_layers=2,embedding_dim=55,l2=0.0000,learning_rate=0.0084,max_epochs_2023-08-11_11-40-12\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_7dada229_2_trial_index=1,batch_size=7,conv_layers=2,embedding_dim=55,l2=0.0000,learning_rate=0.0084,max_epochs_2023-08-11_11-40-13\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_00eddffc_3_trial_index=2,batch_size=7,conv_layers=2,embedding_dim=55,l2=0.0000,learning_rate=0.0084,max_epochs_2023-08-11_11-40-15\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_8ea19386_4_trial_index=3,batch_size=7,conv_layers=2,embedding_dim=55,l2=0.0000,learning_rate=0.0084,max_epochs_2023-08-11_11-40-17\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_c3b4de2c_5_trial_index=4,batch_size=7,conv_layers=2,embedding_dim=55,l2=0.0000,learning_rate=0.0084,max_epochs_2023-08-11_11-40-18\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_c6d1b9ac_6_trial_index=0,batch_size=6,conv_layers=2,embedding_dim=155,l2=0.0000,learning_rate=0.0165,max_epoch_2023-08-11_11-40-20\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_a54411cb_7_trial_index=1,batch_size=6,conv_layers=2,embedding_dim=155,l2=0.0000,learning_rate=0.0165,max_epoch_2023-08-11_11-40-21\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_9aa2e998_8_trial_index=2,batch_size=6,conv_layers=2,embedding_dim=155,l2=0.0000,learning_rate=0.0165,max_epoch_2023-08-11_11-40-23\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_efa240ee_9_trial_index=3,batch_size=6,conv_layers=2,embedding_dim=155,l2=0.0000,learning_rate=0.0165,max_epoch_2023-08-11_11-40-25\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_d35497f1_10_trial_index=4,batch_size=6,conv_layers=2,embedding_dim=155,l2=0.0000,learning_rate=0.0165,max_epoc_2023-08-11_11-40-26\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_ffc43055_11_trial_index=0,batch_size=8,conv_layers=3,embedding_dim=15,l2=0.0000,learning_rate=0.0012,max_epoch_2023-08-11_11-40-28\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_83732aca_12_trial_index=1,batch_size=8,conv_layers=3,embedding_dim=15,l2=0.0000,learning_rate=0.0012,max_epoch_2023-08-11_11-40-30\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_825b6f5e_13_trial_index=2,batch_size=8,conv_layers=3,embedding_dim=15,l2=0.0000,learning_rate=0.0012,max_epoch_2023-08-11_11-40-31\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_5eaa0c6f_14_trial_index=3,batch_size=8,conv_layers=3,embedding_dim=15,l2=0.0000,learning_rate=0.0012,max_epoch_2023-08-11_11-40-33\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_312fad16_15_trial_index=4,batch_size=8,conv_layers=3,embedding_dim=15,l2=0.0000,learning_rate=0.0012,max_epoch_2023-08-11_11-40-34\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_ccacf83d_16_trial_index=0,batch_size=8,conv_layers=5,embedding_dim=205,l2=0.0014,learning_rate=0.0004,max_epoc_2023-08-11_11-40-36\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_a0e8d0ee_17_trial_index=1,batch_size=8,conv_layers=5,embedding_dim=205,l2=0.0014,learning_rate=0.0004,max_epoc_2023-08-11_11-40-38\n",
      "- /home/daviddavo/ray_results/_aux_train_daostack_2023-08-11_11-40-09/_aux_train_daostack_8e909f75_18_trial_index=2,batch_size=8,conv_layers=5,embedding_dim=205,l2=0.0014,learning_rate=0.0004,max_epoc_2023-08-11_11-40-39\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m Worker exits with an exit code None.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m  Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     raise RuntimeError(\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     actor_class = pickle.loads(pickled_class)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     return torch.load(io.BytesIO(b))\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     result = unpickler.load()\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     wrap_storage=restore_location(obj, location),\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     result = fn(storage, location)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m RuntimeError: CUDA error: out of memory\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     raise RuntimeError(\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     actor_class = pickle.loads(pickled_class)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     return torch.load(io.BytesIO(b))\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     result = unpickler.load()\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     wrap_storage=restore_location(obj, location),\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     result = fn(storage, location)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m RuntimeError: CUDA error: out of memory\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m An unexpected internal error occurred while the worker was executing a task.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1498, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     raise RuntimeError(\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     actor_class = pickle.loads(pickled_class)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     return torch.load(io.BytesIO(b))\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     result = unpickler.load()\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     wrap_storage=restore_location(obj, location),\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     result = fn(storage, location)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m RuntimeError: CUDA error: out of memory\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"python/ray/_raylet.pyx\", line 824, in ray._raylet.store_task_errors\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 638, in temporary_actor_method\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     raise RuntimeError(\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/ray/_private/function_manager.py\", line 677, in _load_actor_class_from_gcs\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     actor_class = pickle.loads(pickled_class)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     return torch.load(io.BytesIO(b))\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     result = unpickler.load()\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     wrap_storage=restore_location(obj, location),\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     result = fn(storage, location)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"/home/daviddavo/.local/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m     return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m RuntimeError: CUDA error: out of memory\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1830, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=246768)\u001b[0m SystemExit\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['hostname', 'node_ip', 'logdir', 'should_checkpoint', 'pid'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m exp_df \u001b[38;5;241m=\u001b[39m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhostname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnode_ip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogdir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mshould_checkpoint\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m exp_df\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp@5 test\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py:5399\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5251\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   5252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   5253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5260\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5262\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5263\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5264\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5397\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5398\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5401\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5405\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5406\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5407\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4505\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py:4546\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4544\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4546\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4547\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4549\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py:6934\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6934\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6935\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6936\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['hostname', 'node_ip', 'logdir', 'should_checkpoint', 'pid'] not found in axis\""
     ]
    }
   ],
   "source": [
    "exp_df = exp.get_dataframe().drop(columns=['hostname', 'node_ip', 'logdir', 'should_checkpoint', 'pid'])\n",
    "exp_df.sort_values('p@5 test', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using all of this\n",
    "\n",
    "Crearé una función que reciba una dirección de un usuario y retorne k propuestas que puedan interesarle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(user: str, K: int = 12, ignore_train: bool=False):\n",
    "    uid = encoder_user.transform([user])[0]\n",
    "    print(f\"Recommending {K} proposals for user {user} (uid:{uid}) with {vpu.at[user]} votes\")\n",
    "    \n",
    "    # Getting embedding\n",
    "    out = model(edge_index)\n",
    "    user_embed, item_embed = torch.split(out, (model.n_users, model.n_items))\n",
    "    relevance_score = torch.matmul(user_embed, torch.transpose(item_embed, 0, 1))\n",
    "    if ignore_train:\n",
    "        i = torch.stack([\n",
    "            torch.LongTensor(train_df['uid'].values),\n",
    "            torch.LongTensor(train_df['pid'].values),\n",
    "        ])\n",
    "        v = torch.ones(len(train_df), dtype=torch.float64)\n",
    "        t_interactions = torch.sparse.FloatTensor(i, v, (model.n_users, model.n_items)).to_dense().to(device)\n",
    "        # mask out training user-item interactions from metric computation\n",
    "        # We are only interested in novel items, as a user won't be interested\n",
    "        # in \"voting again\"\n",
    "        relevance_score = torch.mul(relevance_score, (1 - t_interactions))\n",
    "    \n",
    "    topk_relevance_indices = torch.topk(relevance_score, K).indices\n",
    "    \n",
    "    pids = topk_relevance_indices[uid].tolist()\n",
    "    proposals = dfp.loc[encoder_prop.inverse_transform(pids)]\n",
    "    \n",
    "    proposals['userVoted'] = dfv.groupby('proposal')['voter'].apply(lambda x: user in set(x))\n",
    "    \n",
    "    print(f\"precision@{K}={sum(proposals['userVoted'])/len(proposals)*100:.2f}%\")\n",
    "    \n",
    "    return proposals\n",
    "\n",
    "user = \"0x334f12afb7d8740868be04719639616533075234\" # vpu[(12 < vpu) & (vpu < 38)].sample().index[0]\n",
    "recommend(user, ignore_train=True)[['network', 'createdAt', 'title', 'description', 'userVoted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfv[dfv['proposal'] == '0xb92d2df99a47244c07a9d7ef73530c273f1d65230dbff9e95873d82c0314534e']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
