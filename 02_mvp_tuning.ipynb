{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on Pytorch Geometric official example: https://github.com/pyg-team/pytorch_geometric/blob/master/examples/lightgcn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import datetime as dt\n",
    "import itertools as it\n",
    "import functools as ft\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "from tqdm.notebook import tqdm # Progress bars\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "\n",
    "# https://import-as.github.io\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric as PyG\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from ray import tune\n",
    "from ray.air import Checkpoint, session\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import src\n",
    "from src.data import get_df, filter_df\n",
    "from src.graph_utils import shift_edge_indices, unshift_edge_indices\n",
    "from src.neg_sampling import structured_negative_sampling\n",
    "\n",
    "RANDOM_SEED = 1701"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters table in [Google Drive](https://docs.google.com/spreadsheets/d/1riafpWt1563w9pbqdt1g2QZVkc7TfRWGzFaCG5rudDI/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Remove users with less than 6 votes from the dataset before splitting\n",
    "DatasetConfig = namedtuple('DatasetConfig', ('min_votes_per_user', 'allowed_dao_names', 'num_folds'))\n",
    "datasetConfig = DatasetConfig(\n",
    "    min_votes_per_user=6,\n",
    "    allowed_dao_names={'dxDAO', 'xDXdao'},\n",
    "    num_folds=5,\n",
    ")\n",
    "\n",
    "ModelConfig = namedtuple('ModelConfig', 'max_epochs batch_size learning_rate embedding_dim conv_layers l2')\n",
    "modelConfig = ModelConfig(\n",
    "    max_epochs=50,\n",
    "    batch_size=64,\n",
    "    learning_rate=0.001,\n",
    "    embedding_dim=32,\n",
    "    conv_layers=3,\n",
    "    l2=1e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, HeteroData, Data\n",
    "from src.datasets import Daostack\n",
    "\n",
    "def print_graph_stats(g: HeteroData):\n",
    "    density = (g.num_edges) / (g.num_nodes*(g.num_nodes-1))\n",
    "    print(f'Edges:   {g.num_edges:12}')\n",
    "    print(f'Density: {density*100:12.4f}%')\n",
    "\n",
    "data = Daostack(\"./data/dao-analyzer/\", min_vpu=datasetConfig.min_votes_per_user, allowed_daos=datasetConfig.allowed_dao_names)[0]\n",
    "print_graph_stats(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, I thought the RandomLinkSplit function was not working properly, but it turns out that I wasn't understanding it very well. The tutorial I used for [01_mvp](./01_mvp.ipynb) is not very good either, it was written by students, and implemented before PyTorch Geometric bundled the LightGCN model with it.\n",
    "\n",
    "> I think this is totally correct. It seems like you are looking at the shapes of edge_index, while you may want to look at the shapes of edge_label and edge_label_index (which correctly model a 80/10/10 split ratio). Here, edge_index is solely used for message passing, i.e.,\n",
    "> \n",
    "> * for training, we exchange messages on all training edges\n",
    "> * for validation, we exchange messages on all training edges\n",
    "> * for testing, we exchange messages on all training and validation edges\n",
    "> Let me know if this resolves your concerns :)\n",
    ">\n",
    "> -- [Split Error in RandomLinkSplit · Issue #3668 · pyg-team/pytorch_geometric · GitHub](https://github.com/pyg-team/pytorch_geometric/issues/3668)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def graph_k_fold(g: Data | HeteroData, folds, edge_type=None):\n",
    "    skf = StratifiedKFold(folds, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "    folds = []\n",
    "\n",
    "    # Stratify by voter\n",
    "    if edge_type is None:\n",
    "        edge_type = g.edge_types[0]\n",
    "        rev_edge_type = g.edge_types[1]\n",
    "        \n",
    "    edge_index = g[edge_type].edge_index\n",
    "    for train_idx, val_idx in skf.split(torch.zeros(edge_index.size(1)), edge_index[0]):\n",
    "        gtrain = g.edge_subgraph({\n",
    "            edge_type:torch.tensor(train_idx),\n",
    "            rev_edge_type:torch.tensor(train_idx),\n",
    "        })\n",
    "        assert gtrain.is_undirected()\n",
    "        assert len(gtrain[edge_type].edge_index[0].unique()) == len(g[edge_type].edge_index[0].unique())\n",
    "        # The negative samples should be different each epoch\n",
    "        # gtrain[edge_type].negative_samples = structured_negative_sampling(gtrain[edge_type].edge_index, (aux[edge_type[0]].num_nodes, aux[edge_type[2]].num_nodes))[2]\n",
    "        gval = g.edge_subgraph({\n",
    "            edge_type:torch.tensor(val_idx),\n",
    "            rev_edge_type:torch.tensor(val_idx),\n",
    "        })\n",
    "        assert gval.is_undirected()\n",
    "        assert len(gval[edge_type].edge_index[0].unique()) == len(g[edge_type].edge_index[0].unique())\n",
    "        assert (gtrain[edge_type].edge_index[0].unique() == gval[edge_type].edge_index[0].unique()).all()\n",
    "\n",
    "        folds.append((gtrain, gval))\n",
    "\n",
    "    return folds\n",
    "\n",
    "graph_folds = graph_k_fold(data, datasetConfig.num_folds)\n",
    "graph_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the LightGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import LightGCN\n",
    "\n",
    "# Based on:\n",
    "# - https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "# - https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html\n",
    "# - https://github.com/pyg-team/pytorch_geometric/blob/master/examples/lightgcn.py\n",
    "def train_daostack(train: HeteroData, validation: HeteroData, original: HeteroData, modelConfig: ModelConfig, disable_tqdm=False):\n",
    "    if not isinstance(modelConfig, ModelConfig):\n",
    "        modelConfig = ModelConfig(**modelConfig)\n",
    "    \n",
    "    model = LightGCN(\n",
    "        num_nodes=original.num_nodes,\n",
    "        embedding_dim=modelConfig.embedding_dim,\n",
    "        num_layers=modelConfig.conv_layers,\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=modelConfig.learning_rate)\n",
    "\n",
    "    checkpoint = session.get_checkpoint()\n",
    "\n",
    "    if checkpoint:\n",
    "        checkpoint_state = checkpoint.to_dict()\n",
    "        start_epoch = checkpoint_state[\"epoch\"]\n",
    "        model.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    assert train.is_undirected()\n",
    "    assert validation.is_undirected()\n",
    "\n",
    "    # We need to convert the edge indices to homogeneous\n",
    "    # In hetero data the numbers are shared between the node types\n",
    "    # while in homo data they are shifted\n",
    "    original, train, validation = map(shift_edge_indices, [original, train, validation])\n",
    "    \n",
    "    # nodes = torch.arange(0, train.num_nodes, device=device)\n",
    "    users = torch.arange(train['voter'].shift, train['voter'].end, device=device)\n",
    "    items = torch.arange(train['proposal'].shift, train['proposal'].end, device=device)\n",
    "    n_users = train['voter'].num_nodes\n",
    "    n_items = train['proposal'].num_nodes\n",
    "\n",
    "    message_passing_edge_index = torch.concat([s.edge_index for s in train.edge_stores], dim=1)\n",
    "\n",
    "    # The official LightGCN usage also uses this line of code (well, for homo graphs)\n",
    "    # - https://github.com/pyg-team/pytorch_geometric/blob/master/examples/lightgcn.py\n",
    "    # In our case, we will use just voter ---> proposal\n",
    "    train_edge_label_index = train['voter', 'votes', 'proposal'].edge_index\n",
    "    assert (train_edge_label_index[0] < train['voter'].end).all()\n",
    "    assert (train['proposal'].shift <= train_edge_label_index[1]).all()\n",
    "\n",
    "    # TODO: Use LinkLoader instead (i don't know how)\n",
    "    # Waiting for pyg-team/pytorch_geometric#7817\n",
    "    # train_loader = PyG.loader.LinkLoader(\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        range(train_edge_label_index.size(1)), # dataset\n",
    "        batch_size=modelConfig.batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _prec_rec(k: int, remove_training=False):\n",
    "        # gt: ground truth (all edges)\n",
    "        gt_index = original['voter', 'votes', 'proposal'].edge_index\n",
    "        if remove_training:\n",
    "            edge_index = validation['voter', 'votes', 'proposal'].edge_index\n",
    "        else:\n",
    "            # All edges\n",
    "            edge_index = original['voter', 'votes', 'proposal'].edge_index\n",
    "\n",
    "        R = item_count = PyG.utils.degree(gt_index[0], num_nodes=n_users)\n",
    "        # topr.size [104, R.max()]\n",
    "        # TODO: Usar otro edge_index tal vez ponga a 0 la convolución,\n",
    "        # pero no \"descartará\" los items. Hay que calcularlo \"a mano\" usando get_embedding y punto\n",
    "        # además así no dependemos de mi fork\n",
    "        topr = model.recommend(message_passing_edge_index, src_index=users, dst_index=items, k=int(R.max()), sorted=True)\n",
    "        \n",
    "        # assert (model.recommend(edge_index, src_index=users, dst_index=items, k=k) == topk).all()\n",
    "        n_samples = len(users)\n",
    "\n",
    "        # [104, 2216]\n",
    "        ground_truth = torch.full((n_users, n_items), False, dtype=torch.bool, device=device)\n",
    "        ground_truth[gt_index[0], gt_index[1] - original['proposal'].shift] = True\n",
    "\n",
    "        # This is the only line that depends on topr and thus depends on edge_index\n",
    "        isin_rmat = ground_truth.gather(1, topr - original['proposal'].shift)\n",
    "        isin_mat = isin_rmat[:, :k]\n",
    "\n",
    "        prec = (isin_mat.sum(dim=-1) / k).sum() / n_samples\n",
    "        rec = (isin_mat.sum(dim=-1) / item_count).sum() / n_samples\n",
    "\n",
    "        # Now mask isin_rmat to get only up to :R elements\n",
    "        msk = torch.arange(1, R.max()+1, device=device) > R.unsqueeze(1)\n",
    "        isin_rmat[msk] = 0\n",
    "        rprec = (isin_rmat.sum(dim=-1) / R).sum() / n_samples\n",
    "\n",
    "        # print('prec, rec:', (prec, rec))\n",
    "        \n",
    "        return float(prec), float(rec), float(rprec)\n",
    "\n",
    "    for epoch in trange(start_epoch, modelConfig.max_epochs, disable=disable_tqdm):\n",
    "        # index is an array of batch_size that indicates which edges from \n",
    "        # train.edge_index we should use\n",
    "        acc_loss = n_samples = 0        \n",
    "        \n",
    "        for index in tqdm(train_loader, leave=False, delay=1, disable=disable_tqdm):\n",
    "            pos_edge_index = train_edge_label_index[:, index]\n",
    "            # TODO: Change to negative structured sampling like in original LightGCN implementation\n",
    "            neg_edge_index = torch.stack([\n",
    "                pos_edge_index[0],\n",
    "                torch.randint(train['proposal'].shift, train['proposal'].end,\n",
    "                          (pos_edge_index.size(1), ), device=device)\n",
    "            ], dim=0)\n",
    "            \n",
    "            edge_label_index = torch.cat([\n",
    "                pos_edge_index,\n",
    "                neg_edge_index,\n",
    "            ], dim=1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pos_rank, neg_rank = model(message_passing_edge_index, edge_label_index).chunk(2)\n",
    "\n",
    "            # Learning\n",
    "            loss = model.recommendation_loss(\n",
    "                pos_rank,\n",
    "                neg_rank,\n",
    "                node_id=edge_label_index.unique(),\n",
    "                lambda_reg=modelConfig.l2,\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            acc_loss += float(loss) * pos_rank.numel()\n",
    "            n_samples += pos_rank.numel()\n",
    "\n",
    "        checkpoint = Checkpoint.from_dict({\n",
    "            'epoch': epoch,\n",
    "            'net_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        })\n",
    "\n",
    "        prec5, rec5, rprec = _prec_rec(5, remove_training=False)\n",
    "        prec5t, rec5t, rprect = _prec_rec(5, remove_training=True)\n",
    "        session.report({\n",
    "            'loss': acc_loss/n_samples,\n",
    "            'rprec train': rprec, 'rprec test': rprect,\n",
    "            'p@5 train': prec5, 'p@5 test': prec5t,\n",
    "            'r@5 train': rec5, 'r@5 test': rec5t,\n",
    "        }, checkpoint=checkpoint)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Testing just syntax errors\n",
    "model = train_daostack(graph_folds[0][0].to(device), graph_folds[0][1].to(device), data.to(device), ModelConfig(**(modelConfig._asdict() | {'max_epochs':2})))\n",
    "print(PyG.nn.summary(model, data['voter', 'votes', 'proposal'].edge_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search import Repeater\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "\n",
    "def _aux_train_daostack(config):\n",
    "    # TODO: Is bad practice to pass a dataset trainable\n",
    "    # config['embedding_dim'] = 2**config['embedding_dim']\n",
    "    config['batch_size'] = 2**config['batch_size']\n",
    "    n_fold = config.pop('__trial_index__')\n",
    "    train, validation = graph_folds[n_fold]\n",
    "    return train_daostack(train.to(device), validation.to(device), data.to(device), config, disable_tqdm=True)\n",
    "\n",
    "tryConfigs = ModelConfig(\n",
    "    max_epochs=50,\n",
    "    conv_layers=tune.randint(2,6),\n",
    "    learning_rate=tune.qloguniform(1e-5, 1, 1e-5),\n",
    "    l2=tune.loguniform(1e-9, 1e-1),\n",
    "    # These will be 2 to the power\n",
    "    batch_size=tune.randint(4,10), # 16..1024\n",
    "    # embedding_dim=tune.randint(4,8), # 16..128\n",
    "    embedding_dim=tune.qlograndint(10, 500, 5),\n",
    ")\n",
    "\n",
    "# It is recommended to not use Repeater with a TrialScheduler. Early termination can negatively affect the average reported metric.\n",
    "asha_scheduler = None\n",
    "# asha_scheduler = ASHAScheduler(\n",
    "#     time_attr='training_iteration',\n",
    "#     max_t=50,\n",
    "#     grace_period=5,\n",
    "#     reduction_factor=3,\n",
    "#     brackets=1,\n",
    "# )\n",
    "\n",
    "search_alg = HyperOptSearch()\n",
    "search_alg = Repeater(search_alg,datasetConfig.num_folds)\n",
    "\n",
    "# Every run takes approx half a gig of vram (no optimizations)\n",
    "# The RTX 4090 has 24GB so we can run the model about 48 times\n",
    "resources_per_trial={\n",
    "    'cpu': 1,\n",
    "    'memory': 0 if torch.cuda.is_available() else 2e9,\n",
    "    # 'gpu': 1/32 if torch.cuda.is_available() else 0,\n",
    "    'gpu': torch.cuda.is_available(),\n",
    "}\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(_aux_train_daostack, resources_per_trial),\n",
    "    param_space=tryConfigs._asdict(),\n",
    "    tune_config=tune.TuneConfig(\n",
    "        # time_budget_s=60,\n",
    "        num_samples=datasetConfig.num_folds*1,\n",
    "        scheduler=asha_scheduler,\n",
    "        search_alg=search_alg,\n",
    "        metric='rprec test',\n",
    "        mode='max',\n",
    "    )\n",
    ")\n",
    "exp = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_df = exp.get_dataframe().drop(columns=['hostname', 'node_ip', 'logdir', 'should_checkpoint', 'pid'])\n",
    "exp_df.sort_values('p@5 test', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using all of this\n",
    "\n",
    "Crearé una función que reciba una dirección de un usuario y retorne k propuestas que puedan interesarle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(user: str, K: int = 12, ignore_train: bool=False):\n",
    "    uid = encoder_user.transform([user])[0]\n",
    "    print(f\"Recommending {K} proposals for user {user} (uid:{uid}) with {vpu.at[user]} votes\")\n",
    "    \n",
    "    # Getting embedding\n",
    "    out = model(edge_index)\n",
    "    user_embed, item_embed = torch.split(out, (model.n_users, model.n_items))\n",
    "    relevance_score = torch.matmul(user_embed, torch.transpose(item_embed, 0, 1))\n",
    "    if ignore_train:\n",
    "        i = torch.stack([\n",
    "            torch.LongTensor(train_df['uid'].values),\n",
    "            torch.LongTensor(train_df['pid'].values),\n",
    "        ])\n",
    "        v = torch.ones(len(train_df), dtype=torch.float64)\n",
    "        t_interactions = torch.sparse.FloatTensor(i, v, (model.n_users, model.n_items)).to_dense().to(device)\n",
    "        # mask out training user-item interactions from metric computation\n",
    "        # We are only interested in novel items, as a user won't be interested\n",
    "        # in \"voting again\"\n",
    "        relevance_score = torch.mul(relevance_score, (1 - t_interactions))\n",
    "    \n",
    "    topk_relevance_indices = torch.topk(relevance_score, K).indices\n",
    "    \n",
    "    pids = topk_relevance_indices[uid].tolist()\n",
    "    proposals = dfp.loc[encoder_prop.inverse_transform(pids)]\n",
    "    \n",
    "    proposals['userVoted'] = dfv.groupby('proposal')['voter'].apply(lambda x: user in set(x))\n",
    "    \n",
    "    print(f\"precision@{K}={sum(proposals['userVoted'])/len(proposals)*100:.2f}%\")\n",
    "    \n",
    "    return proposals\n",
    "\n",
    "user = \"0x334f12afb7d8740868be04719639616533075234\" # vpu[(12 < vpu) & (vpu < 38)].sample().index[0]\n",
    "recommend(user, ignore_train=True)[['network', 'createdAt', 'title', 'description', 'userVoted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfv[dfv['proposal'] == '0xb92d2df99a47244c07a9d7ef73530c273f1d65230dbff9e95873d82c0314534e']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "033786b15c70420bb412e5fbfb414a44": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "072101fa63d548208748b29a352d254c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0c45aa8ab172427ab1d83cfa726a3c7d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0f310daf7e034b1eafc96354f2087ac1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "14777276025b45cf802ca6151fdeaeb7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_9fd02d1bab0b40f38c617543f4f49025",
       "max": 104,
       "style": "IPY_MODEL_a664a23811704067a9498419da2def29",
       "value": 104
      }
     },
     "1dd6f66799a043028372d6b546973fe7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_36b16b0f790c43b99c3bdb5be525c0bf",
       "style": "IPY_MODEL_557caa45ef1249429fecd44da2348277",
       "value": " 98/104 [00:01&lt;00:00, 64.10it/s]"
      }
     },
     "25132776ea654990b340336077718580": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_67b04cb1254f48b1bd6539ba20eb32d3",
       "max": 2,
       "style": "IPY_MODEL_35f2afab57094e64bdf0ced05729628c",
       "value": 2
      }
     },
     "28ec774ea589493abe99ce6b7e370c99": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "35f2afab57094e64bdf0ced05729628c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "36b16b0f790c43b99c3bdb5be525c0bf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "441412da067948df93a13f4b19ed864a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "502e920f8a2447909dd8859ef3d06e0d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_0c45aa8ab172427ab1d83cfa726a3c7d",
       "style": "IPY_MODEL_c86efc7c1e964184ab48f2414bb46d68",
       "value": "100%"
      }
     },
     "557caa45ef1249429fecd44da2348277": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "56bef30cc0114ab5b0dc230aa7b100ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_db280abd121448f28598ae14f55b64f0",
       "style": "IPY_MODEL_0f310daf7e034b1eafc96354f2087ac1",
       "value": " 104/104 [00:01&lt;00:00, 60.30it/s]"
      }
     },
     "56fb6ed818fb4221978c04a1c97b7ac7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "67b04cb1254f48b1bd6539ba20eb32d3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6c20fa773061484f9333c62a5e3070e3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_d7f1f844c27f460e8c5fc31a71955df2",
        "IPY_MODEL_25132776ea654990b340336077718580",
        "IPY_MODEL_b8e7f1d0a0ec407c9ba01128ba15fc0a"
       ],
       "layout": "IPY_MODEL_8f8bc310e926492e8f8059186d2b63c5"
      }
     },
     "8f84d586d62a42a98fbadb59a407feda": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8f8bc310e926492e8f8059186d2b63c5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9693a05afb724108b045be31baae2a2f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9fd02d1bab0b40f38c617543f4f49025": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a5c4519a980d4cc99046c99c80cbe6ec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "a664a23811704067a9498419da2def29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b1bfa0387e16497492e48fc3fb3e9c8a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "b8e7f1d0a0ec407c9ba01128ba15fc0a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_8f84d586d62a42a98fbadb59a407feda",
       "style": "IPY_MODEL_56fb6ed818fb4221978c04a1c97b7ac7",
       "value": " 2/2 [00:03&lt;00:00,  1.73s/it]"
      }
     },
     "c1b9b27ad0274af78401f8ebbdae7150": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_033786b15c70420bb412e5fbfb414a44",
       "max": 104,
       "style": "IPY_MODEL_ded240f2732441ad97b0b583d8ed42eb",
       "value": 104
      }
     },
     "c86efc7c1e964184ab48f2414bb46d68": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d4fa40d1c69b4f59beeeeb3c33209690": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_072101fa63d548208748b29a352d254c",
       "style": "IPY_MODEL_441412da067948df93a13f4b19ed864a",
       "value": " 94%"
      }
     },
     "d7f1f844c27f460e8c5fc31a71955df2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_28ec774ea589493abe99ce6b7e370c99",
       "style": "IPY_MODEL_9693a05afb724108b045be31baae2a2f",
       "value": "100%"
      }
     },
     "db280abd121448f28598ae14f55b64f0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ded240f2732441ad97b0b583d8ed42eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
